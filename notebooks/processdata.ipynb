{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33bc713-08e3-4dc2-896a-4cf64f7f0d08",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "840c7ae4-44e2-4f15-861e-c2fdb95020e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import csv\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ebb6e-53ab-42ed-963a-9f166794e78a",
   "metadata": {},
   "source": [
    "# Read Training Data & Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8311569-9fec-45f2-8596-a94ae6845042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person : 19\n",
      "person : 26\n",
      "person : 27\n",
      "person : 32\n",
      "person : 39\n",
      "person : text.txt\n",
      "6859\n",
      "0.9998542061525003/100     \r"
     ]
    }
   ],
   "source": [
    "def read_training_data():\n",
    "    root = \"TrainingData\\\\\"\n",
    "    raw = []\n",
    "\n",
    "    # Read the audio file\n",
    "    for person in os.listdir(root):\n",
    "        print(\"person :\", person)\n",
    "        person_path = os.path.join(root, person)\n",
    "        \n",
    "        if os.path.isdir(person_path):\n",
    "            for sub_dir in os.listdir(person_path):\n",
    "                new_path = os.path.join(person_path, sub_dir)\n",
    "                \n",
    "                if os.path.isdir(new_path):\n",
    "                    for audio_file in os.listdir(new_path):\n",
    "                        audio_path = os.path.join(new_path, audio_file)\n",
    "                        \n",
    "                        if audio_file.endswith(('.wav', '.mp3', '.ogg', '.flac')):\n",
    "                            y, sr = librosa.load(audio_path)\n",
    "                            buf, buf_cur = 0, sr\n",
    "                            \n",
    "                            while buf + buf_cur < len(y):                     \n",
    "                                new_y = y[buf : buf + buf_cur]\n",
    "                                raw.append([person, new_y, sr])\n",
    "                                buf += buf_cur\n",
    "\n",
    "    # raw_data is data, but not crop the audio length\n",
    "    data = []\n",
    "    print(len(raw))\n",
    "    cnt = 0\n",
    "    for id, y, sr in raw:\n",
    "        print(\"{}\".format(cnt), \"\\r\", end=\"\")\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=512)\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        beat_frames = librosa.frames_to_time(librosa.util.fix_frames(librosa.onset.onset_detect(y=y, sr=sr)))\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        \n",
    "        data.append([id, mel_spec, mfccs, tempo, beat_frames, zcr, centroid, chroma, rms])\n",
    "        cnt = cnt+1\n",
    "\n",
    "    col = [\"id\", \"mel_spec\", \"mfccs\", \"tempo\", \"beat_frames\", \"zcr\", \"centroid\", \"chroma\", \"rms\"]\n",
    "    df = pd.DataFrame(data, columns=col)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = read_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cac69c07-dfad-456e-89b5-54d93041e531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dimension input is the same\n",
      "id double\n",
      "mel_spec (128, 44)\n",
      "mfccs (13, 44)\n",
      "tempo ()\n",
      "beat_frames (5,)\n",
      "zcr (1, 44)\n",
      "centroid (1, 44)\n",
      "chroma (12, 44)\n",
      "rms (1, 44)\n"
     ]
    }
   ],
   "source": [
    "df.to_pickle(\"new_data.pkl\")\n",
    "\n",
    "def check_size(df):\n",
    "    for i in range(df.shape[0]):\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                if df[col][i].shape != df[col][0].shape and col != \"beat_frames\":\n",
    "                    print(\"err :\", col, df[col][i].shape, df[col][0].shape)\n",
    "                    return\n",
    "            except:\n",
    "                continue\n",
    "    print(\"All dimension input is the same\")\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            print(col, df[col][0].shape)\n",
    "        except:\n",
    "            print(col, \"double\")\n",
    "        \n",
    "\n",
    "check_size(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cbafba-53bb-4ab3-a631-3ddc09160cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
